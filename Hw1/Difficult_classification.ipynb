{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from mp1 import generate_dataset_classification\n",
    "from mp1 import generate_test_set_classification\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten\n",
    "from pandas import get_dummies\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[x_train, t_train] = generate_dataset_classification(1000, 20, True)\n",
    "t_train = get_dummies(t_train).values\n",
    "\n",
    "[x_test, t_test] = generate_test_set_classification()\n",
    "t_test = get_dummies(t_test).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with 1-layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ------------\n",
      "Epoch 1/80\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 1.2270 - acc: 0.3600\n",
      "Epoch 2/80\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 1.0854 - acc: 0.4160\n",
      "Epoch 3/80\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.9996 - acc: 0.4920\n",
      "Epoch 4/80\n",
      "1000/1000 [==============================] - 0s 250us/step - loss: 0.9594 - acc: 0.5000\n",
      "Epoch 5/80\n",
      "1000/1000 [==============================] - 0s 254us/step - loss: 0.9207 - acc: 0.5320\n",
      "Epoch 6/80\n",
      "1000/1000 [==============================] - 0s 254us/step - loss: 0.9068 - acc: 0.5410\n",
      "Epoch 7/80\n",
      "1000/1000 [==============================] - 0s 264us/step - loss: 0.8759 - acc: 0.5690\n",
      "Epoch 8/80\n",
      "1000/1000 [==============================] - 0s 268us/step - loss: 0.8647 - acc: 0.5640\n",
      "Epoch 9/80\n",
      "1000/1000 [==============================] - 0s 283us/step - loss: 0.8388 - acc: 0.5800\n",
      "Epoch 10/80\n",
      "1000/1000 [==============================] - 0s 270us/step - loss: 0.8390 - acc: 0.5840\n",
      "Epoch 11/80\n",
      "1000/1000 [==============================] - 0s 257us/step - loss: 0.8149 - acc: 0.6090\n",
      "Epoch 12/80\n",
      "1000/1000 [==============================] - 0s 266us/step - loss: 0.8284 - acc: 0.5940\n",
      "Epoch 13/80\n",
      "1000/1000 [==============================] - 0s 269us/step - loss: 0.8081 - acc: 0.6030\n",
      "Epoch 14/80\n",
      "1000/1000 [==============================] - 0s 268us/step - loss: 0.7999 - acc: 0.6000\n",
      "Epoch 15/80\n",
      "1000/1000 [==============================] - 0s 267us/step - loss: 0.7991 - acc: 0.5920\n",
      "Epoch 16/80\n",
      "1000/1000 [==============================] - 0s 272us/step - loss: 0.7813 - acc: 0.6240\n",
      "Epoch 17/80\n",
      "1000/1000 [==============================] - 0s 262us/step - loss: 0.7820 - acc: 0.6280\n",
      "Epoch 18/80\n",
      "1000/1000 [==============================] - 0s 264us/step - loss: 0.8198 - acc: 0.5950\n",
      "Epoch 19/80\n",
      "1000/1000 [==============================] - 0s 279us/step - loss: 0.7632 - acc: 0.6350\n",
      "Epoch 20/80\n",
      "1000/1000 [==============================] - 0s 267us/step - loss: 0.7872 - acc: 0.5980\n",
      "Epoch 21/80\n",
      "1000/1000 [==============================] - 0s 327us/step - loss: 0.7439 - acc: 0.6350\n",
      "Epoch 22/80\n",
      "1000/1000 [==============================] - 0s 332us/step - loss: 0.8374 - acc: 0.5810\n",
      "Epoch 23/80\n",
      "1000/1000 [==============================] - 0s 391us/step - loss: 0.7634 - acc: 0.6270\n",
      "Epoch 24/80\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.7418 - acc: 0.6340\n",
      "Epoch 25/80\n",
      "1000/1000 [==============================] - 0s 288us/step - loss: 0.7836 - acc: 0.5990\n",
      "Epoch 26/80\n",
      "1000/1000 [==============================] - 0s 252us/step - loss: 0.7293 - acc: 0.6460\n",
      "Epoch 27/80\n",
      "1000/1000 [==============================] - 0s 244us/step - loss: 0.7791 - acc: 0.6100\n",
      "Epoch 28/80\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.7237 - acc: 0.6480\n",
      "Epoch 29/80\n",
      "1000/1000 [==============================] - 0s 244us/step - loss: 0.7507 - acc: 0.6350\n",
      "Epoch 30/80\n",
      "1000/1000 [==============================] - 0s 355us/step - loss: 0.7291 - acc: 0.6360\n",
      "Epoch 31/80\n",
      "1000/1000 [==============================] - 0s 348us/step - loss: 0.7406 - acc: 0.6370\n",
      "Epoch 32/80\n",
      "1000/1000 [==============================] - 0s 312us/step - loss: 0.7280 - acc: 0.6430\n",
      "Epoch 33/80\n",
      "1000/1000 [==============================] - 0s 268us/step - loss: 0.7137 - acc: 0.6550\n",
      "Epoch 34/80\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.7108 - acc: 0.6660\n",
      "Epoch 35/80\n",
      "1000/1000 [==============================] - 0s 261us/step - loss: 0.7263 - acc: 0.6580\n",
      "Epoch 36/80\n",
      "1000/1000 [==============================] - 0s 296us/step - loss: 0.7365 - acc: 0.6360\n",
      "Epoch 37/80\n",
      "1000/1000 [==============================] - 0s 318us/step - loss: 0.6986 - acc: 0.6700\n",
      "Epoch 38/80\n",
      "1000/1000 [==============================] - 0s 388us/step - loss: 0.7120 - acc: 0.6530\n",
      "Epoch 39/80\n",
      "1000/1000 [==============================] - 0s 313us/step - loss: 0.6970 - acc: 0.6440\n",
      "Epoch 40/80\n",
      "1000/1000 [==============================] - 0s 252us/step - loss: 0.7216 - acc: 0.6560\n",
      "Epoch 41/80\n",
      "1000/1000 [==============================] - 0s 249us/step - loss: 0.7230 - acc: 0.6350\n",
      "Epoch 42/80\n",
      "1000/1000 [==============================] - 0s 252us/step - loss: 0.7355 - acc: 0.6430\n",
      "Epoch 43/80\n",
      "1000/1000 [==============================] - 0s 291us/step - loss: 0.6979 - acc: 0.6540\n",
      "Epoch 44/80\n",
      "1000/1000 [==============================] - 0s 313us/step - loss: 0.6781 - acc: 0.6710\n",
      "Epoch 45/80\n",
      "1000/1000 [==============================] - 0s 274us/step - loss: 0.6751 - acc: 0.6610\n",
      "Epoch 46/80\n",
      "1000/1000 [==============================] - 0s 305us/step - loss: 0.6934 - acc: 0.6520\n",
      "Epoch 47/80\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.6754 - acc: 0.6680\n",
      "Epoch 48/80\n",
      "1000/1000 [==============================] - 0s 260us/step - loss: 0.6805 - acc: 0.6600\n",
      "Epoch 49/80\n",
      "1000/1000 [==============================] - 0s 241us/step - loss: 0.6965 - acc: 0.6720\n",
      "Epoch 50/80\n",
      "1000/1000 [==============================] - 0s 273us/step - loss: 0.6839 - acc: 0.6620\n",
      "Epoch 51/80\n",
      "1000/1000 [==============================] - 0s 267us/step - loss: 0.6943 - acc: 0.6700\n",
      "Epoch 52/80\n",
      "1000/1000 [==============================] - 0s 280us/step - loss: 0.6909 - acc: 0.6640\n",
      "Epoch 53/80\n",
      "1000/1000 [==============================] - 0s 308us/step - loss: 0.6679 - acc: 0.6800\n",
      "Epoch 54/80\n",
      "1000/1000 [==============================] - 0s 251us/step - loss: 0.6736 - acc: 0.6860\n",
      "Epoch 55/80\n",
      "1000/1000 [==============================] - 0s 272us/step - loss: 0.6729 - acc: 0.6690\n",
      "Epoch 56/80\n",
      "1000/1000 [==============================] - 0s 244us/step - loss: 0.6805 - acc: 0.6770\n",
      "Epoch 57/80\n",
      "1000/1000 [==============================] - 0s 279us/step - loss: 0.6884 - acc: 0.6520\n",
      "Epoch 58/80\n",
      "1000/1000 [==============================] - 0s 268us/step - loss: 0.6734 - acc: 0.6560\n",
      "Epoch 59/80\n",
      "1000/1000 [==============================] - 0s 266us/step - loss: 0.6617 - acc: 0.6860\n",
      "Epoch 60/80\n",
      "1000/1000 [==============================] - 0s 254us/step - loss: 0.6808 - acc: 0.6580\n",
      "Epoch 61/80\n",
      "1000/1000 [==============================] - 0s 241us/step - loss: 0.6569 - acc: 0.6760\n",
      "Epoch 62/80\n",
      "1000/1000 [==============================] - 0s 250us/step - loss: 0.6754 - acc: 0.6640\n",
      "Epoch 63/80\n",
      "1000/1000 [==============================] - 0s 238us/step - loss: 0.7141 - acc: 0.6470\n",
      "Epoch 64/80\n",
      "1000/1000 [==============================] - 0s 242us/step - loss: 0.6591 - acc: 0.6800\n",
      "Epoch 65/80\n",
      "1000/1000 [==============================] - 0s 234us/step - loss: 0.6617 - acc: 0.6730\n",
      "Epoch 66/80\n",
      "1000/1000 [==============================] - 0s 243us/step - loss: 0.6503 - acc: 0.6730\n",
      "Epoch 67/80\n",
      "1000/1000 [==============================] - 0s 249us/step - loss: 0.6544 - acc: 0.6830\n",
      "Epoch 68/80\n",
      "1000/1000 [==============================] - 0s 240us/step - loss: 0.6873 - acc: 0.6540\n",
      "Epoch 69/80\n",
      "1000/1000 [==============================] - 0s 238us/step - loss: 0.6502 - acc: 0.6890\n",
      "Epoch 70/80\n",
      "1000/1000 [==============================] - 0s 261us/step - loss: 0.6701 - acc: 0.6720\n",
      "Epoch 71/80\n",
      "1000/1000 [==============================] - 0s 260us/step - loss: 0.6439 - acc: 0.6910\n",
      "Epoch 72/80\n",
      "1000/1000 [==============================] - 0s 271us/step - loss: 0.6578 - acc: 0.6640\n",
      "Epoch 73/80\n",
      "1000/1000 [==============================] - 0s 243us/step - loss: 0.6440 - acc: 0.6800\n",
      "Epoch 74/80\n",
      "1000/1000 [==============================] - 0s 240us/step - loss: 0.6522 - acc: 0.6850\n",
      "Epoch 75/80\n",
      "1000/1000 [==============================] - 0s 251us/step - loss: 0.6435 - acc: 0.6830\n",
      "Epoch 76/80\n",
      "1000/1000 [==============================] - 0s 270us/step - loss: 0.6620 - acc: 0.6810\n",
      "Epoch 77/80\n",
      "1000/1000 [==============================] - 0s 275us/step - loss: 0.6596 - acc: 0.6570\n",
      "Epoch 78/80\n",
      "1000/1000 [==============================] - 0s 264us/step - loss: 0.6213 - acc: 0.7090\n",
      "Epoch 79/80\n",
      "1000/1000 [==============================] - 0s 265us/step - loss: 0.6610 - acc: 0.6790\n",
      "Epoch 80/80\n",
      "1000/1000 [==============================] - 0s 265us/step - loss: 0.6461 - acc: 0.6900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e3864d630>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn = Sequential([\n",
    "        Dense(3, input_dim=10000),\n",
    "        Activation('softmax'),\n",
    "    \n",
    "    #Dense(1000, input_dim=10000),\n",
    "    #Activation('relu'),\n",
    "    #Dense(500),\n",
    "    #Activation('relu'),\n",
    "    #Dense(100),\n",
    "    #Activation('relu'),\n",
    "    #Dense(3),\n",
    "    #Activation('softmax'),\n",
    "])\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "dnn.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "print('Training ------------')\n",
    "dnn.fit(x_train, t_train, epochs=80, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 386us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.83611810366312667, 0.59666666825612391]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn.evaluate(x_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ------------\n",
      "Epoch 1/40\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1.0798 - acc: 0.4460\n",
      "Epoch 2/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7982 - acc: 0.6280\n",
      "Epoch 3/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7137 - acc: 0.6550\n",
      "Epoch 4/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6729 - acc: 0.6830\n",
      "Epoch 5/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6250 - acc: 0.6900\n",
      "Epoch 6/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6009 - acc: 0.7250\n",
      "Epoch 7/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5907 - acc: 0.7070\n",
      "Epoch 8/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5643 - acc: 0.7300\n",
      "Epoch 9/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5463 - acc: 0.7490\n",
      "Epoch 10/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5229 - acc: 0.7730\n",
      "Epoch 11/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5055 - acc: 0.7760\n",
      "Epoch 12/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4850 - acc: 0.8100\n",
      "Epoch 13/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4695 - acc: 0.7920\n",
      "Epoch 14/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4457 - acc: 0.8120\n",
      "Epoch 15/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4380 - acc: 0.8170\n",
      "Epoch 16/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4089 - acc: 0.8320\n",
      "Epoch 17/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3978 - acc: 0.8520\n",
      "Epoch 18/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3808 - acc: 0.8450\n",
      "Epoch 19/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3567 - acc: 0.8710\n",
      "Epoch 20/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3472 - acc: 0.8720\n",
      "Epoch 21/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3241 - acc: 0.8920\n",
      "Epoch 22/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3052 - acc: 0.8980\n",
      "Epoch 23/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2944 - acc: 0.9040\n",
      "Epoch 24/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2694 - acc: 0.9050\n",
      "Epoch 25/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2577 - acc: 0.9120\n",
      "Epoch 26/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2419 - acc: 0.9340\n",
      "Epoch 27/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2286 - acc: 0.9350\n",
      "Epoch 28/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2160 - acc: 0.9420\n",
      "Epoch 29/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2049 - acc: 0.9390\n",
      "Epoch 30/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1995 - acc: 0.9410\n",
      "Epoch 31/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1851 - acc: 0.9500\n",
      "Epoch 32/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1762 - acc: 0.9480\n",
      "Epoch 33/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1725 - acc: 0.9530\n",
      "Epoch 34/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1664 - acc: 0.9550\n",
      "Epoch 35/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1582 - acc: 0.9570\n",
      "Epoch 36/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1416 - acc: 0.9690\n",
      "Epoch 37/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1410 - acc: 0.9620\n",
      "Epoch 38/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1291 - acc: 0.9750\n",
      "Epoch 39/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1246 - acc: 0.9760\n",
      "Epoch 40/40\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1263 - acc: 0.9710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e415be5c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = Sequential()  \n",
    "cnn.add(Conv2D(filters=16,  \n",
    "               kernel_size=(5,5),  \n",
    "               padding='same',  \n",
    "               input_shape=(100,100,1),  \n",
    "               activation='relu'))  \n",
    "cnn.add(MaxPool2D(pool_size=(2,2)))  \n",
    "cnn.add(Conv2D(filters=36,  \n",
    "                 kernel_size=(5,5),  \n",
    "                 padding='same',  \n",
    "                 input_shape=(28,28,1),  \n",
    "                 activation='relu'))  \n",
    "cnn.add(MaxPool2D(pool_size=(2,2)))  \n",
    "cnn.add(Flatten())  \n",
    "cnn.add(Dense(128, activation='relu'))  \n",
    "cnn.add(Dense(3, activation='softmax'))  \n",
    "cnn.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "print('Training ------------')\n",
    "cnn.fit(np.reshape(x_train, (-1, 100, 100, 1)), t_train, epochs=40, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 761us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4133899966875712, 0.82333333492279048]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.evaluate(np.reshape(x_test, (-1, 100, 100, 1)), t_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
